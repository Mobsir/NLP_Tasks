import asyncio
import cv2
import time
import sounddevice as sd
from scipy.io.wavfile import write
import speech_recognition as sr
import edge_tts
import os
import pygame
import unicodedata
from datetime import datetime



# Folders to store images
FAMILY_FOLDER = "family"
CAPTURE_FOLDER = "captured_images"
for folder in [FAMILY_FOLDER, CAPTURE_FOLDER]:
    if not os.path.exists(folder):
        os.makedirs(folder)

# Initialize pygame mixer for playing TTS audio
pygame.mixer.init()

def normalize_text(text):
     """
    Normalize Arabic text by removing diacritics and converting to lowercase.

    This function uses Unicode normalization to strip combining characters (harakat)
    and then returns the clean, lowercased text.

    Args:
        text (str): Arabic text to normalize.

    Returns:
        str: Normalized text without diacritics and in lowercase.
    """
    text = unicodedata.normalize('NFKD', text)
    text = ''.join(c for c in text if not unicodedata.combining(c))
    return text.lower().strip()


# List of recognized voice commands (normalized)
START_COMMANDS = [normalize_text(cmd) for cmd in ["Ø£ÙÙ‡Ù’Ù„Ù‹Ø§ Ù…ÙØ¨Ù’ØµÙØ±", "Ù…ÙØ±Ù’Ø­ÙØ¨Ù‹Ø§ Ù…ÙØ¨Ù’ØµÙØ±", "Ø§Ù„Ø³ÙÙ‘Ù„ÙØ§Ù…Ù Ø¹ÙÙ„ÙÙŠÙ’Ùƒ"]]
EXPLORE_COMMANDS = [normalize_text(cmd) for cmd in ["Ø§ÙØ³Ù’ØªÙÙƒÙ’Ø´ÙÙÙ’ Ø§Ù„Ù…ÙÙƒÙØ§Ù†", "Ø§ÙØ³Ù’ØªÙÙƒÙ’Ø´ÙØ§Ù Ø§Ù„Ù…ÙÙƒÙØ§Ù†", "Ø§ÙØ³Ù’ØªÙÙƒÙ’Ø´ÙØ§Ù"]]
PHOTO_COMMANDS = [normalize_text(cmd) for cmd in ["Ø§ÙÙ„Ù’ØªÙÙ‚ÙØ·Ù’ ØµÙÙˆØ±ÙØ©", "ØµÙÙˆÙÙ‘Ø±Ù’", "Ø£ÙØ®ÙØ°Ù’ ØµÙÙˆØ±ÙØ©"]]
EXIT_COMMANDS = [normalize_text(cmd) for cmd in ["Ø´ÙÙƒÙ’Ø±Ù‹Ø§ Ù…ÙØ¨Ù’ØµÙØ±", "Ø¥ÙÙ†Ù’Ù‡ÙØ§Ø¡", "Ø®ÙØ±ÙÙˆØ¬"]]

# Text-to-Speech using Edge TTS
async def edge_speak(text):
    """
    Convert Arabic text to speech using Edge TTS and play it.

    This function creates an MP3 file using Edge TTS with an Arabic voice,
    plays it using pygame, and then deletes the file after playback.

    Args:
        text (str): Arabic text to speak.
    """
    voice = "ar-EG-SalmaNeural"
    filename = "temp.mp3"
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(filename)

    try:
        pygame.mixer.music.load(filename)
        pygame.mixer.music.play()
        while pygame.mixer.music.get_busy():
            pygame.time.Clock().tick(10)
    finally:
        pygame.mixer.music.unload()
        if os.path.exists(filename):
            os.remove(filename)

# Speech recognition
def listen_once(duration=3, fs=16000):
    """
    Record a short audio clip from the microphone and transcribe it to text.

    Uses Google Speech Recognition (with Arabic language) to convert speech to text.
    Normalizes the result before returning.

    Args:
        duration (int, optional): Recording duration in seconds. Default is 3.
        fs (int, optional): Sample rate in Hz. Default is 16000.

    Returns:
        str: Normalized recognized text, or an empty string if recognition fails.
    """
    print("ğŸ¤ ÙŠÙØ³Ù’ØªÙÙ…ÙØ¹Ù...")
    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
    sd.wait()
    filename = "temp.wav"
    write(filename, fs, recording)

    recognizer = sr.Recognizer()
    with sr.AudioFile(filename) as source:
        audio = recognizer.record(source)
    try:
        text = recognizer.recognize_google(audio, language="ar-EG")
        print(f"ğŸ”Š Ù‚ÙÙ„Ù’ØªÙ: {text}")
        return normalize_text(text)
    except sr.UnknownValueError:
        print("âŒ Ù„ÙÙ…Ù’ Ø£ÙÙÙ’Ù‡ÙÙ…Ù Ø§Ù„ÙƒÙÙ„ÙØ§Ù…Ù")
        return ""
    except sr.RequestError as e:
        print(f"âŒ Ø®ÙØ·ÙØ£ÙŒ ÙÙÙŠ Ø®ÙØ¯Ù’Ù…ÙØ©Ù Ø§Ù„ØªÙÙ‘Ø¹ÙØ±ÙÙ‘ÙÙ: {e}")
        return ""
    finally:
        if os.path.exists(filename):
            os.remove(filename)

# Image capture functions

def capture_Family_image():
    """
    Capture an image and save it to the 'family' folder with a custom name.

    Prompts the user to enter a filename, checks for duplicates, and saves
    the captured photo using OpenCV.

    Returns:
        str or None: File path of the saved image if successful, or None if failed.
    """
    print("ğŸ“¸ Ø³ÙÙŠÙØªÙÙ…ÙÙ‘ Ø§Ù„ØªÙÙ‘ØµÙ’ÙˆÙÙŠØ±Ù Ø¨ÙØ¹Ù’Ø¯Ù Ù£ Ø«ÙÙˆÙØ§Ù†Ù... Ø§ÙØ¨Ù’ØªÙØ³ÙÙ…Ù’ ğŸ˜Š")
    time.sleep(3)

    try:
        cam = cv2.VideoCapture(0)
        if not cam.isOpened():
            raise Exception("ğŸ“· Ù„ÙÙ…Ù’ Ø£ÙØªÙÙ…ÙÙƒÙÙ‘Ù†Ù’ Ù…ÙÙ†Ù’ ÙÙØªÙ’Ø­Ù Ø§Ù„ÙƒÙØ§Ù…ÙÙŠØ±ÙØ§")

        ret, frame = cam.read()
        cam.release()

        if not ret:
            raise Exception("ğŸ“· Ù„ÙÙ…Ù’ Ø£ÙØªÙÙ…ÙÙƒÙÙ‘Ù†Ù’ Ù…ÙÙ†Ù’ Ø§Ù„ØªÙÙ‘Ù‚ÙØ§Ø·Ù Ø§Ù„ØµÙÙ‘ÙˆØ±ÙØ©")

        while True:
            img_name_input = input("ğŸ“ Ø£ÙØ¯Ù’Ø®ÙÙ„Ù Ø§Ø³Ù’Ù…Ù Ø§Ù„ØµÙÙ‘ÙˆØ±ÙØ©Ù (Ø¨Ø¯ÙˆÙ† Ø§Ù…ØªØ¯Ø§Ø¯): ").strip()

            if img_name_input:
                img_name_clean = "".join(c for c in img_name_input if c.isalnum())
                if not img_name_clean:
                    print("âŒ Ù‡ÙØ°ÙØ§ Ø§Ù„Ø§ÙØ³Ù’Ù…Ù ØºÙÙŠÙ’Ø±Ù ØµÙØ§Ù„ÙØ­ÙØŒ Ø¬ÙØ±ÙÙ‘Ø¨Ù Ø§Ø³Ù’Ù…Ù‹Ø§ Ø¢Ø®ÙØ±Ù.")
                    continue

                img_path = os.path.join(FOLDER_NAME, f"{img_name_clean}.png")

                if os.path.exists(img_path):
                    print("âš ï¸ Ø§Ù„ØµÙÙ‘ÙˆØ±ÙØ©Ù Ù…ÙÙˆÙ’Ø¬ÙÙˆØ¯ÙØ©ÙŒ Ø¨ÙØ§Ù„ÙÙØ¹Ù’Ù„Ù. Ø§ÙØ®Ù’ØªÙØ±Ù Ø§Ø³Ù’Ù…Ù‹Ø§ Ø¢Ø®ÙØ±Ù.")
                else:
                    break
            else:
                print("âŒ Ù„ÙÙ…Ù’ ØªÙØ¯Ù’Ø®ÙÙ„Ù Ø£ÙÙŠÙÙ‘ Ø§Ø³Ù’Ù…ÙØŒ Ø¬ÙØ±ÙÙ‘Ø¨Ù’ Ù…ÙØ±ÙÙ‘Ø©Ù‹ Ø£ÙØ®Ù’Ø±ÙÙ‰.")

        cv2.imwrite(img_path, frame)
        print("âœ… ØªÙÙ…ÙÙ‘ Ø§Ù„ØªÙÙ‘Ù‚ÙØ§Ø·Ù Ø§Ù„ØµÙÙ‘ÙˆØ±ÙØ©Ù.")
        return img_path

    except Exception as e:
        print(f"âŒ Ø®ÙØ·ÙØ£ÙŒ: {e}")
        return None


def capture_image():
    """
    Capture an image and save it to the 'captured_images' folder using a unique timestamp.

    Uses OpenCV to take the photo, waits for 3 seconds before capturing, and
    saves the image file with a timestamp-based name.

    Returns:
        str or None: File path of the saved image if successful, or None if failed.
    """
    print("ğŸ“¸ Ø³ÙÙŠÙØªÙÙ…ÙÙ‘ Ø§Ù„ØªÙÙ‘ØµÙ’ÙˆÙÙŠØ±Ù Ø¨ÙØ¹Ù’Ø¯Ù Ù£ Ø«ÙÙˆÙØ§Ù†Ù... Ø§ÙØ¨Ù’ØªÙØ³ÙÙ…Ù’ ğŸ˜Š")
    time.sleep(3)

    try:
        cam = cv2.VideoCapture(0)
        if not cam.isOpened():
            raise Exception("ğŸ“· Ù„ÙÙ…Ù’ Ø£ÙØªÙÙ…ÙÙƒÙÙ‘Ù†Ù’ Ù…ÙÙ†Ù’ ÙÙØªÙ’Ø­Ù Ø§Ù„ÙƒÙØ§Ù…ÙÙŠØ±ÙØ§")

        ret, frame = cam.read()
        cam.release()

        if not ret:
            raise Exception("ğŸ“· Ù„ÙÙ…Ù’ Ø£ÙØªÙÙ…ÙÙƒÙÙ‘Ù†Ù’ Ù…ÙÙ†Ù’ Ø§Ù„ØªÙÙ‘Ù‚ÙØ§Ø·Ù Ø§Ù„ØµÙÙ‘ÙˆØ±ÙØ©")

        # Ù†Ø³ØªØ®Ø¯Ù… timestamp ÙƒØ§Ø³Ù… Ù„Ù„Ù…Ù„Ù Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙØ±Ø¯
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        img_filename = f"image_{timestamp}.png"
        img_path = os.path.join(FOLDER_NAME, img_filename)

        # Ù†Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©
        cv2.imwrite(img_path, frame)

        print(f"âœ… ØªÙÙ…ÙÙ‘ Ø§Ù„ØªÙÙ‘Ù‚ÙØ§Ø·Ù Ø§Ù„ØµÙÙ‘ÙˆØ±ÙØ©Ù ÙˆÙØªÙØ®Ù’Ø²ÙÙŠÙ†ÙÙ‡ÙØ§: {img_path}")
        return img_path

    except Exception as e:
        print(f"âŒ Ø®ÙØ·ÙØ£ÙŒ: {e}")
        return None












# def start_point():
#     print("Ù…ÙØ¨Ù’ØµÙØ±Ù ÙŠÙÙ‚ÙÙˆÙ„Ù: Ø£ÙÙ‡Ù’Ù„Ù‹Ø§ Ø¨ÙÙƒÙ! Ù‚ÙÙ„Ù’: Ø£ÙÙ‡Ù’Ù„Ù‹Ø§ Ù…ÙØ¨Ù’ØµÙØ± Ù„ÙÙ†ÙØ¨Ù’Ø¯ÙØ£.")
#     asyncio.run(edge_speak("Ø£ÙÙ‡Ù’Ù„Ù‹Ø§ Ø¨ÙÙƒÙ! Ø£ÙÙ†ÙØ§ Ù…ÙØ¨Ù’ØµÙØ±. Ù‚ÙÙ„Ù’: Ø£ÙÙ‡Ù’Ù„Ù‹Ø§ Ù…ÙØ¨Ù’ØµÙØ± Ù„ÙÙ†ÙØ¨Ù’Ø¯ÙØ£."))

#     while True:
#         command = listen_once(duration=3)
#         if any(word in command for word in START_COMMANDS):
#             asyncio.run(edge_speak("ØªÙÙ…ÙØ§Ù…ÙŒ! ÙŠÙÙ…Ù’ÙƒÙÙ†ÙÙƒÙ Ù‚ÙÙˆÙ’Ù„Ù: Ø§ÙØ³Ù’ØªÙÙƒÙ’Ø´ÙÙÙ’ Ø§Ù„Ù…ÙÙƒÙØ§Ù†ØŒ Ø§ÙÙ„Ù’ØªÙÙ‚ÙØ·Ù’ ØµÙÙˆØ±ÙØ©ØŒ Ø£ÙÙˆÙ’ Ø´ÙÙƒÙ’Ø±Ù‹Ø§ Ù…ÙØ¨Ù’ØµÙØ±."))
#             break
#         elif command:
#             asyncio.run(edge_speak("Ù„ÙÙ…Ù’ Ø£ÙØ³Ù’Ù…ÙØ¹Ù’: Ø£ÙÙ‡Ù’Ù„Ù‹Ø§ Ù…ÙØ¨Ù’ØµÙØ±ØŒ Ø­ÙØ§ÙˆÙÙ„Ù’ Ù…ÙØ±ÙÙ‘Ø©Ù‹ Ø£ÙØ®Ù’Ø±ÙÙ‰."))

#     while True:
#         command = listen_once(duration=3)
#         if any(word in command for word in EXPLORE_COMMANDS):
#             explore_place()
#         elif any(word in command for word in PHOTO_COMMANDS):
#             img_path = capture_image()
#             if img_path:
#                 asyncio.run(edge_speak(f"ØªÙÙ…ÙÙ‘ Ø§Ù„ØªÙÙ‘Ù‚ÙØ§Ø·Ù Ø§Ù„ØµÙÙ‘ÙˆØ±ÙØ©Ù "))
#             else:
#                 asyncio.run(edge_speak("ÙˆÙÙ‚ÙØ¹ÙØªÙ’ Ù…ÙØ´Ù’ÙƒÙÙ„ÙØ©ÙŒ Ø£ÙØ«Ù’Ù†ÙØ§Ø¡Ù Ø§Ù„ØªÙÙ‘ØµÙ’ÙˆÙÙŠØ±Ù"))
#         elif any(word in command for word in EXIT_COMMANDS):
#             asyncio.run(edge_speak("Ø¥ÙÙ„ÙÙ‰ Ø§Ù„Ù„ÙÙ‘Ù‚ÙØ§Ø¡Ù!"))
#             break
#         elif command:
#             asyncio.run(edge_speak("Ù„ÙÙ…Ù’ Ø£ÙÙÙ’Ù‡ÙÙ…Ù Ø§Ù„Ø·ÙÙ‘Ù„ÙØ¨ÙØŒ Ø£ÙØ¹ÙØ¯Ù Ø§Ù„Ù…ÙØ­ÙØ§ÙˆÙÙ„ÙØ©Ù."))





# # if __name__ == "__main__":
# #     try:
# #         start_point()
# #     finally:
# #         pygame.mixer.quit()
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
# import os
# import cv2
# import torch
# import sounddevice as sd
# from transformers import VitsModel, AutoTokenizer
# import speech_recognition as sr

# print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
# model = VitsModel.from_pretrained("facebook/mms-tts-ara")
# tokenizer = AutoTokenizer.from_pretrained("facebook/mms-tts-ara")
# print(" ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")

# def speak(text):
#     inputs = tokenizer(text, return_tensors="pt")
#     with torch.no_grad():
#         output = model(**inputs).waveform
#     wav = output.squeeze().cpu().numpy()
#     sd.play(wav, samplerate=model.config.sampling_rate)
#     sd.wait()
#     print(f"(Ù…Ø¨ØµØ± Ù‚Ø§Ù„): {text}")

# # Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ù…Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
# def recognize_speech():
#     recognizer = sr.Recognizer()
#     with sr.Microphone() as source:
#         print("Ø§Ø³ØªÙ…Ø¹...")
#         recognizer.adjust_for_ambient_noise(source, duration=1)
#         try:
#             audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
#             command = recognizer.recognize_google(audio, language='ar-EG')
#             print(f"Google Ø³Ù…Ø¹: {command}")
#             return command.lower()
#         except sr.UnknownValueError:
#             speak("Ù„Ù… Ø£ÙÙ‡Ù…. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
#             return ""
#         except sr.RequestError as e:
#             speak("Ø®Ø·Ø£ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù….")
#             return ""
#         except sr.WaitTimeoutError:
#             speak("Ù„Ù… Ø£Ø³Ù…Ø¹ Ø£ÙŠ Ø´ÙŠØ¡.")
#             return ""

# def ensure_family_folder():
#     if not os.path.exists("family"):
#         os.makedirs("family")

# def capture_photo():
#     ensure_family_folder()
#     cap = cv2.VideoCapture(0)
#     if not cap.isOpened():
#         speak("ØªØ¹Ø°Ø± ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")
#         print("ØªØ¹Ø°Ø± ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")
#         return
#     speak("Ø§Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§. Ø³ÙŠØªÙ… Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¹Ø¯ Ø«Ù„Ø§Ø« Ø«ÙˆØ§Ù†.")
#     for i in range(3,0,-1):
#         print(i)
#     ret, frame = cap.read()
#     if ret:
#         name = input("Ø§ÙƒØªØ¨ Ø§Ø³Ù… Ø§Ù„Ø´Ø®Øµ: ")
#         filename = f"family/{name}.jpg"
#         cv2.imwrite(filename, frame)
#         speak(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³Ù… {filename}")
#         print(f" ØµÙˆØ±Ø© Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ {filename}")
#     else:
#         speak("ØªØ¹Ø°Ø± Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ØµÙˆØ±Ø©.")
#     cap.release()
#     cv2.destroyAllWindows()

# def detection_mode():
#     cap = cv2.VideoCapture(0)
#     if not cap.isOpened():
#         speak("ØªØ¹Ø°Ø± ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")
#         print("ØªØ¹Ø°Ø± ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")
#         return
#     speak("ØªÙ… ØªÙØ¹ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„ÙƒØ´Ù. Ø§Ø¶ØºØ· Q Ù„Ù„Ø®Ø±ÙˆØ¬.")
#     while True:
#         ret, frame = cap.read()
#         if not ret:
#             break
#         cv2.imshow("ÙƒØ´Ù", frame)
#         if cv2.waitKey(1) & 0xFF == ord('q'):
#             break
#     cap.release()
#     cv2.destroyAllWindows()

# def main():
#     speak("Ù…Ø±Ø­Ø¨Ù‹Ø§ØŒ Ø£Ù†Ø§ Ù…Ø¨ØµØ±. Ù‚Ù„ Ø§Ù‡Ù„Ø§ Ù…Ø¨ØµØ± Ù„Ù„Ø¨Ø¯Ø¡.")
#     while True:
#         command = recognize_speech()
#         if "Ø§Ù‡Ù„Ø§ Ù…Ø¨ØµØ±" in command:
#             speak("Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ù‚Ù„ ÙƒØ´Ù Ø£Ùˆ ØµÙˆØ±Ø© Ø£Ùˆ Ø´ÙƒØ±Ø§ Ù…Ø¨ØµØ± Ù„Ù„Ø®Ø±ÙˆØ¬.")
#             while True:
#                 action = recognize_speech()
#                 print(f"Google Ø³Ù…Ø¹: {action}")
#                 if any(word in action for word in ["ÙƒØ´Ù", "Ø§Ù„ÙƒØ´Ù", "ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§", "Ø§ÙØªØ­ ÙƒØ§Ù…ÙŠØ±Ø§", "Ø§ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"]):
#                     detection_mode()
#                 elif any(word in action for word in ["ØµÙˆØ±Ø©", "Ø§Ù„ØªØµÙˆÙŠØ±", "ØµÙˆØ±", "ØªØµÙˆÙŠØ±"]):
#                     capture_photo()
#                 elif any(word in action for word in ["Ø´ÙƒØ±Ø§ Ù…Ø¨ØµØ±", "Ø´ÙƒØ±Ø§ ÙŠØ§ Ù…Ø¨ØµØ±", "Ø®Ø±ÙˆØ¬", "Ø§Ù†Ù‡Ø§Ø¡"]):
#                     speak("Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©!")
#                     return
#                 elif action != "":
#                     speak("Ù„Ù… Ø£ÙÙ‡Ù…. Ù‚Ù„ ÙƒØ´Ù Ø£Ùˆ ØµÙˆØ±Ø© Ø£Ùˆ Ø´ÙƒØ±Ø§ Ù…Ø¨ØµØ±.")

# if __name__ == "__main__":
#     main()


# =========================================================================================




